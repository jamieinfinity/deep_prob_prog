{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# dSprites Warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset_zip = np.load('/home/jupyter/causalML/tutorials/data/dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz', allow_pickle=True, encoding='bytes')\n",
    "\n",
    "imgs = dataset_zip['imgs']\n",
    "latents_values = dataset_zip['latents_values']\n",
    "latents_classes = dataset_zip['latents_classes']\n",
    "metadata = dataset_zip['metadata'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'date', b'description', b'version', b'latents_names', b'latents_possible_values', b'latents_sizes', b'author', b'title'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Define number of values per latents and functions to convert to indices\n",
    "latents_sizes = metadata[b'latents_sizes']\n",
    "latents_bases = np.concatenate((latents_sizes[::-1].cumprod()[::-1][1:],\n",
    "                                np.array([1,])))\n",
    "\n",
    "def latent_to_index(latents):\n",
    "    return np.dot(latents, latents_bases).astype(int)\n",
    "\n",
    "def sample_latent(size=1):\n",
    "    samples = np.zeros((size, latents_sizes.size))\n",
    "    for lat_i, lat_size in enumerate(latents_sizes):\n",
    "        samples[:, lat_i] = np.random.randint(lat_size, size=size)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Helper function to show images\n",
    "def show_images_grid(imgs_, num_images=25):\n",
    "    ncols = int(np.ceil(num_images**0.5))\n",
    "    nrows = int(np.ceil(num_images / ncols))\n",
    "    _, axes = plt.subplots(ncols, nrows, figsize=(nrows * 3, ncols * 3))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax_i, ax in enumerate(axes):\n",
    "        if ax_i < num_images:\n",
    "            ax.imshow(imgs_[ax_i], cmap='Greys_r',  interpolation='nearest')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    else:\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 60000 # 60000\n",
    "latents_sampled = sample_latent(size=SAMPLE_SIZE)\n",
    "# latents_sampled[:, 1] = 1\n",
    "\n",
    "# Select images\n",
    "indices_sampled = latent_to_index(latents_sampled)\n",
    "imgs_sampled = imgs[indices_sampled]\n",
    "\n",
    "# Show images\n",
    "# show_images_grid(imgs_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(imgs_sampled.astype('float32'), latents_sampled[:,1].astype('int64'), \n",
    "                                                    test_size=0.1667, random_state=42)\n",
    "x_train, y_train, x_test, y_test = map(\n",
    "    torch.tensor, (x_train, y_train, x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 64\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.05\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_data(train_ds, test_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(test_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, IMAGE_SIZE, IMAGE_SIZE).to(dev), y.to(dev)\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "    \n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "    \n",
    "def fit(epochs, model, loss_func, opt, train_dl, test_dl):\n",
    "    # loop through epochs\n",
    "    for epoch in range(epochs):\n",
    "        # loop through all batches, adjusts weights via opt and loss_func\n",
    "        model.train() # put model in training mode (for things like dropout)\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        # calculate validation loss\n",
    "        model.eval() # put model in evaluation mode (for things like dropout)\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in test_dl]\n",
    "            )\n",
    "        test_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "test_ds = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_dl, test_dl = get_data(train_ds, test_ds, BATCH_SIZE)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "test_dl = WrappedDataLoader(test_dl, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class ModelDSprite(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64*16*16, 64)\n",
    "        self.fc2 = nn.Linear(64, 3)        \n",
    "    def forward(self, x):\n",
    "        # 32 x 64 x 64\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # 32 x 32 x 32\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0)\n",
    "        # 64 x 32 x 32\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # 64 x 16 x 16\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0)\n",
    "        x = x.view(-1, 64*16*16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9742710431965083\n",
      "1 0.9754475372787762\n",
      "2 0.9618331156499718\n",
      "3 0.7237962132643471\n",
      "4 0.2568916223736149\n",
      "5 0.039183057822291555\n",
      "6 0.013970756335297577\n",
      "7 0.011966080526796919\n",
      "8 0.01564020741985315\n",
      "9 0.004673301935988506\n",
      "10 0.00551016409068531\n",
      "11 0.002877418363012092\n",
      "12 0.002784902598635122\n",
      "13 0.002708173279665966\n",
      "14 0.002717289106368756\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "model = ModelDSprite()\n",
    "model.to(dev)\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "opt = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "fit(EPOCHS, model, loss_func, opt, train_dl, test_dl)\n",
    "\n",
    "# 0 1.0205792663121696\n",
    "# 1 1.0181344623471278\n",
    "# 2 1.0312850714516482\n",
    "# 3 0.9798258404640217\n",
    "# 4 0.9714416380620818\n",
    "# 5 0.9638791082859325\n",
    "# 6 1.026337376667771\n",
    "# 7 0.6992057580109764\n",
    "# 8 0.37132105935671117\n",
    "# 9 0.13169585563014494\n",
    "# CPU times: user 15min 12s, sys: 5min 55s, total: 21min 7s\n",
    "# Wall time: 7min 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9993, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def count_correct(y_pred, y_truth):\n",
    "    preds = torch.argmax(y_pred, dim=1)\n",
    "    return (preds == y_truth).float().sum()\n",
    "\n",
    "model.eval()\n",
    "cc = 0\n",
    "tot = 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        y_pred = model(xb)\n",
    "        tot += len(y_pred)\n",
    "        cc += count_correct(y_pred, yb)\n",
    "accuracy = cc/tot\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = np.random.choice(range(len(y_test)))\n",
    "pred = model(x_test[ind].view(-1,1,IMAGE_SIZE,IMAGE_SIZE).to(dev))\n",
    "pred = torch.argmax(pred,dim=1)\n",
    "print(pred[0].item(), '  ', y_test[ind].item())\n",
    "plt.imshow(x_test[ind].reshape((IMAGE_SIZE, IMAGE_SIZE)), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Save / load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"models\")\n",
    "MNIST_MODEL_PATH = MODEL_PATH / \"dsprite\"\n",
    "\n",
    "MNIST_MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "FILENAME = (MNIST_MODEL_PATH / \"dsprite_classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# save model state\n",
    "torch.save(model.state_dict(), FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_saved = nn.Sequential(\n",
    "#     nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.AdaptiveAvgPool2d(1),\n",
    "#     Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "# )\n",
    "# model_saved.to(dev)\n",
    "\n",
    "model_saved = ModelDSprite()\n",
    "model_saved.to(dev)\n",
    "\n",
    "model_saved.load_state_dict(torch.load(FILENAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9993, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def count_correct(y_pred, y_truth):\n",
    "    preds = torch.argmax(y_pred, dim=1)\n",
    "    return (preds == y_truth).float().sum()\n",
    "\n",
    "model_saved.eval()\n",
    "cc = 0\n",
    "tot = 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        y_pred = model_saved(xb)\n",
    "        tot += len(y_pred)\n",
    "        cc += count_correct(y_pred, yb)\n",
    "accuracy = cc/tot\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAMuUlEQVR4nO3dX6wc5X3G8e9TG5c0ARlDQBaGGiQrJRfBRBYlIqoITSKXRoELqIhSya1Qz00qEbVSAq3UNpUqlZtAL6pKFtD4og1Q0sSIixLLAbVXBvOvMXEck5SCZRe3ApSkF6iGXy92XB1Oz2HXuzu7x+f9fqTVzrye3fnJs8++78zOmUlVIWnt+4V5FyBpNgy71AjDLjXCsEuNMOxSIwy71IiJwp5kZ5IjSV5Octe0ipI0fRn3d/Yk64AfAZ8BjgHPAF+oqh9MrzxJ07J+gtdeC7xcVT8BSPIQcDOwYtiTeAaP1LOqynLtkwzjLwVeWzR/rGuTtApN0rMv9+3x/3ruJAvAwgTrkTQFk4T9GHDZovktwPGlC1XVbmA3OIyX5mmSYfwzwLYkVyTZANwOPDadsiRN29g9e1WdSvL7wBPAOuDBqnppapVJmqqxf3oba2UO46Xe9XE0XtJZxLBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41YmjYkzyY5GSSQ4vaNiXZl+Ro93xBv2VKmtQoPfs3gJ1L2u4C9lfVNmB/Ny9pFRsa9qr6Z+CNJc03A3u66T3ALVOuS9KUjbvPfklVnQDoni+eXkmS+jD2LZtHlWQBWOh7PZLe37g9++tJNgN0zydXWrCqdlfVjqraMea6JE3BuGF/DNjVTe8C9k6nHEl9SVW9/wLJN4EbgIuA14E/Bb4DPAJcDrwK3FZVSw/iLfde778ySROrqizXPjTs02TYpf6tFHbPoJMaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaMTTsSS5L8mSSw0leSnJn174pyb4kR7vnC/ovV9K4RrnX22Zgc1U9l+Q84FngFuB3gDeq6i+T3AVcUFVfHfJe3v5J6tnYt3+qqhNV9Vw3/TPgMHApcDOwp1tsD4MvAEmr1BntsyfZClwDHAAuqaoTMPhCAC6ednGSpmf9qAsm+RDwLeDLVfXTZNmRwnKvWwAWxitP0rSMdMvmJOcAjwNPVNXXu7YjwA1VdaLbr3+qqj4y5H3cZ5d6NvY+ewZd+APA4dNB7zwG7OqmdwF7Jy1SUn9GORr/SeBfgO8D73bNf8Rgv/0R4HLgVeC2qnpjyHvZs0s9W6lnH2kYPy2GXerf2MN4SWuDYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWrEKPd6OzfJ00leTPJSkq917VckOZDkaJKHk2zov1xJ4xqlZ38buLGqrga2AzuTXAfcA9xbVduAN4E7+itT0qSGhr0Gft7NntM9CrgReLRr3wPc0kuFmrmqOuOHVr+R9tmTrEvyAnAS2Af8GHirqk51ixwDLu2nREnTMFLYq+qdqtoObAGuBa5abrHlXptkIcnBJAfHL1PSpM7oaHxVvQU8BVwHbEyyvvunLcDxFV6zu6p2VNWOSQqVNJlRjsZ/OMnGbvoDwKeBw8CTwK3dYruAvX0Vqemb9v63+/CrX4ZtmCQfY3AAbh2DL4dHqurPk1wJPARsAp4Hfruq3h7yXn4KVom+A5mk1/fXyqpq2f/8oWGfJsO+ehj2tWulsK9frlFrj0Nrebqs1AjDLjXCYfwa5tBdi9mzS40w7FIjDLvUCMMuNcKwS40w7FIj/OltjfHnNq3Enl1qhGGXGmHYpUa4z66p8E9aVz97dqkRhl1qhMP4s5w/tWlU9uxSIwy71AiH8RqbR+DPLvbsUiMMu9QIwy41wrBLjRg57N1tm59P8ng3f0WSA0mOJnk4yYb+ypQ0qTPp2e9kcEPH0+4B7q2qbcCbwB3TLEzSdI0U9iRbgN8E7u/mA9wIPNotsge4pY8CtXokec9DZ5dRe/b7gK8A73bzFwJvVdWpbv4YcOmUa5M0RaPcn/1zwMmqenZx8zKLLnuSdpKFJAeTHByzRklTMMoZdNcDn09yE3AucD6Dnn5jkvVd774FOL7ci6tqN7AbvGWzNE9De/aquruqtlTVVuB24HtV9UXgSeDWbrFdwN7eqtTcuI++dkzyO/tXgT9I8jKDffgHplOSpD5kln8P7TB++vrefvboZ5+qWnaj+VdvZ7mlYZxG+A342uTpslIjDLvUCIfxa4xDcK3Enl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qxEhXl03yCvAz4B3gVFXtSLIJeBjYCrwC/FZVvdlPmZImdSY9+6eqantV7ejm7wL2V9U2YH83L2mVmmQYfzOwp5veA9wyeTmS+jJq2Av4bpJnkyx0bZdU1QmA7vniPgqUNB2j3hHm+qo6nuRiYF+SH466gu7LYWHogpJ6dca3bE7yZ8DPgd8DbqiqE0k2A09V1UeGvNZbNks9W+mWzUOH8Uk+mOS809PAZ4FDwGPArm6xXcDe6ZQqqQ9De/YkVwLf7mbXA39fVX+R5ELgEeBy4FXgtqp6Y8h72bNLPVupZz/jYfwkDLvUv7GH8ZLWBsMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUiJHCnmRjkkeT/DDJ4SSfSLIpyb4kR7vnC/ouVtL4Ru3Z/wr4p6r6FeBq4DBwF7C/qrYB+7t5SavUKDd2PB94EbiyFi2c5AjeslladSa519uVwH8Cf5vk+ST3d7duvqSqTnRvfgK4eGrVSpq6UcK+Hvg48DdVdQ3w35zBkD3JQpKDSQ6OWaOkKRgl7MeAY1V1oJt/lEH4X++G73TPJ5d7cVXtrqodVbVjGgVLGs/QsFfVfwCvJTm9P/7rwA+Ax4BdXdsuYG8vFUqaiqEH6ACSbAfuBzYAPwF+l8EXxSPA5cCrwG1V9caQ9/EAndSzlQ7QjRT2aTHsUv8mORovaQ0w7FIjDLvUCMMuNcKwS40w7FIjDLvUiPUzXt9/Af8OXNRNz9NqqAGsYynreK8zreOXV/qHmZ5U838rTQ7O+1z51VCDdVjHLOtwGC81wrBLjZhX2HfPab2LrYYawDqWso73mlodc9lnlzR7DuOlRsw07El2JjmS5OUkM7sabZIHk5xMcmhR28wvhZ3ksiRPdpfjfinJnfOoJcm5SZ5O8mJXx9e69iuSHOjqeDjJhj7rWFTPuu76ho/Pq44kryT5fpIXTl9CbU6fkd4u2z6zsCdZB/w18BvAR4EvJPnojFb/DWDnkrZ5XAr7FPCHVXUVcB3wpe7/YNa1vA3cWFVXA9uBnUmuA+4B7u3qeBO4o+c6TruTweXJT5tXHZ+qqu2Lfuqax2ekv8u2V9VMHsAngCcWzd8N3D3D9W8FDi2aPwJs7qY3A0dmVcuiGvYCn5lnLcAvAc8Bv8rg5I31y22vHte/pfsA3wg8DmROdbwCXLSkbabbBTgf+De6Y2nTrmOWw/hLgdcWzR/r2uZlrpfCTrIVuAY4MI9auqHzCwwuFLoP+DHwVlWd6haZ1fa5D/gK8G43f+Gc6ijgu0meTbLQtc16u/R62fZZhn25S+U0+VNAkg8B3wK+XFU/nUcNVfVOVW1n0LNeC1y13GJ91pDkc8DJqnp2cfOs6+hcX1UfZ7Cb+aUkvzaDdS410WXbh5ll2I8Bly2a3wIcn+H6lxrpUtjTluQcBkH/u6r6x3nWAlBVbwFPMTiGsDHJ6b+XmMX2uR74fJJXgIcYDOXvm0MdVNXx7vkk8G0GX4Cz3i4TXbZ9mFmG/RlgW3ekdQNwO4PLUc/LzC+FnSTAA8Dhqvr6vGpJ8uEkG7vpDwCfZnAg6Eng1lnVUVV3V9WWqtrK4PPwvar64qzrSPLBJOedngY+Cxxixtul+r5se98HPpYcaLgJ+BGD/cM/nuF6vwmcAP6HwbfnHQz2DfcDR7vnTTOo45MMhqT/CrzQPW6adS3Ax4DnuzoOAX/StV8JPA28DPwD8Isz3EY3AI/Po45ufS92j5dOfzbn9BnZDhzsts13gAumVYdn0EmN8Aw6qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRvwvMhG1PiPcqIwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = np.random.choice(range(len(y_test)))\n",
    "pred = model_saved(x_test[ind].view(-1,1,IMAGE_SIZE,IMAGE_SIZE).to(dev))\n",
    "pred = torch.argmax(pred,dim=1)\n",
    "print(pred[0].item(), '  ', y_test[ind].item())\n",
    "plt.imshow(x_test[ind].reshape((IMAGE_SIZE, IMAGE_SIZE)), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
